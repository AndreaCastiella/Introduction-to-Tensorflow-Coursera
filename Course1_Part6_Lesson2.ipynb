{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course1-Part6-Lesson2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YD8GIJRBE4cK"
      ],
      "authorship_tag": "ABX9TyMJ+Mw2pAhr4fsC1+PAPMrY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaCastiella/Introduction-to-Tensorflow-Coursera/blob/main/Course1_Part6_Lesson2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD8GIJRBE4cK"
      },
      "source": [
        "#Improving Computer Vision Accuracy using Convolutions\r\n",
        "\r\n",
        "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sizes of hidden layer, number of training epochs etc on the final accuracy.\r\n",
        "\r\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hxro1ApE40p"
      },
      "source": [
        "import tensorflow as tf\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m24Sp4uFFF7",
        "outputId": "3e716670-9ffe-489a-9471-fad1d26c8982"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images = training_images/255.0\r\n",
        "test_images = test_images/255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veBwukU1Fc-o",
        "outputId": "23e46271-c7fd-4228-d46c-45a1756bbeba"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6231 - acc: 0.7842\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3842 - acc: 0.8620\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3366 - acc: 0.8761\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3188 - acc: 0.8841\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2995 - acc: 0.8900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe79c9828d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U0PHZfyGJGp",
        "outputId": "c89247ae-b250-40db-edb9-1c3c11c1016e"
      },
      "source": [
        "model.evaluate(test_images, test_labels, verbose=2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 0s - loss: 0.3453 - acc: 0.8767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.34528878331184387, 0.8766999840736389]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPZhz2eAGU3q"
      },
      "source": [
        "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions.\r\n",
        "\r\n",
        "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\r\n",
        "\r\n",
        "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\r\n",
        "\r\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\r\n",
        "\r\n",
        "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9puE6d3xG1Sy",
        "outputId": "7fac4b20-c9d8-44f1-d2db-15e54584ce5c"
      },
      "source": [
        "training_images.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-q3k-AUG9ow"
      },
      "source": [
        "We need (60000, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrd9V8MRGVh_",
        "outputId": "f1107c15-5484-49fd-ded7-7e3b7104bdd4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images=training_images / 255.0\r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images=test_images/255.0\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\r\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n",
        "model.summary()\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 113,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 39s 20ms/step - loss: 0.6542 - acc: 0.7612\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 38s 20ms/step - loss: 0.3300 - acc: 0.8792\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 38s 20ms/step - loss: 0.2803 - acc: 0.8964\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 38s 20ms/step - loss: 0.2499 - acc: 0.9065\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 38s 20ms/step - loss: 0.2243 - acc: 0.9171\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.2781 - acc: 0.8984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk4v9TL3HKgC"
      },
      "source": [
        "It's likely gone up to about 93% on the training data and 91% on the validation data. \r\n",
        "\r\n",
        "That's significant, and a step in the right direction!\r\n",
        "\r\n",
        "Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \r\n",
        "\r\n",
        "(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\r\n",
        "\r\n",
        "Then, look at the code again, and see, step by step how the Convolutions were built:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUFagtxHLGX"
      },
      "source": [
        "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "import tensorflow as tf\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images=training_images / 255.0\r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images=test_images/255.0\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSQvCvliHbpq"
      },
      "source": [
        "Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\r\n",
        "\r\n",
        "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\r\n",
        "2. The size of the Convolution, in this case a 3x3 grid\r\n",
        "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\r\n",
        "4. In the first layer, the shape of the input data.\r\n",
        "\r\n",
        "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\r\n",
        "\r\n",
        "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. \r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\r\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnOq79GhHczU"
      },
      "source": [
        "Add another convolution\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFch3KBBHpjy"
      },
      "source": [
        "Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\r\n",
        "\r\n",
        "```\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPxq0QliHr2n"
      },
      "source": [
        "Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\r\n",
        "print(test_acc)\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lliFW7gAHtqL"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\r\n",
        "\r\n",
        "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnNnc8yMHvyb",
        "outputId": "e368688e-f3c1-41c0-f3e9-b5f63b2c227c"
      },
      "source": [
        "print(test_labels[:100])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "nSqYPsmHIf48",
        "outputId": "807ed8b3-308c-473f-c636-978cfe28e846"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "f, axarr = plt.subplots(3,4)\r\n",
        "FIRST_IMAGE=0\r\n",
        "SECOND_IMAGE=7\r\n",
        "THIRD_IMAGE=26\r\n",
        "CONVOLUTION_NUMBER = 0\r\n",
        "from tensorflow.keras import models\r\n",
        "layer_outputs = [layer.output for layer in model.layers]\r\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\r\n",
        "for x in range(0,4):\r\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\r\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\r\n",
        "  axarr[0,x].grid(False)\r\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\r\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\r\n",
        "  axarr[1,x].grid(False)\r\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\r\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\r\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwamorsCI5i0"
      },
      "source": [
        "EXERCISES\r\n",
        "\r\n",
        "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\r\n",
        "\r\n",
        "2. Remove the final Convolution. What impact will this have on accuracy or training time?\r\n",
        "\r\n",
        "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\r\n",
        "\r\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \r\n",
        "\r\n",
        "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ-VnGt5I6g7",
        "outputId": "65647b3c-05c8-46f1-ae05-fe31f90a57a2"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images=training_images / 255.0\r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images=test_images/255.0\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\r\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n",
        "model.summary()\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 11, 11, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               51328     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 55,098\n",
            "Trainable params: 55,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.7004 - acc: 0.7532\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3614 - acc: 0.8681\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3046 - acc: 0.8880\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2740 - acc: 0.8999\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2486 - acc: 0.9074\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.2915 - acc: 0.8923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqWwRWt8KreG",
        "outputId": "36c90f14-3b1a-4ab6-b638-bdc0e0de8e67"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images=training_images / 255.0\r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images=test_images/255.0\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\r\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n",
        "model.summary()\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 0.6210 - acc: 0.7738\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 0.3097 - acc: 0.8870\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 0.2564 - acc: 0.9041\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.2206 - acc: 0.9187\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.1957 - acc: 0.9262\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2494 - acc: 0.9104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzbYM4a4MFw1",
        "outputId": "a91b517d-b71e-418e-96b3-eae511e636ff"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images=training_images / 255.0\r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images=test_images/255.0\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\r\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n",
        "model.summary()\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 5408)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               692352    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 693,962\n",
            "Trainable params: 693,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.5137 - acc: 0.8210\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.2678 - acc: 0.9017\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.2177 - acc: 0.9207\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.1779 - acc: 0.9336\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.1514 - acc: 0.9437\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.2496 - acc: 0.9168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BZwlAIDM4CM",
        "outputId": "f91fdcc6-7d8f-4276-9d17-5bd0bd8feb29"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images=training_images / 255.0\r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images=test_images/255.0\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\r\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\r\n",
        "model.summary()\r\n",
        "model.fit(training_images, training_labels, epochs=5)\r\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 3, 3, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               4224      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 24,330\n",
            "Trainable params: 24,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 41s 21ms/step - loss: 0.8521 - acc: 0.6852\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 40s 21ms/step - loss: 0.4573 - acc: 0.8334\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 40s 21ms/step - loss: 0.3880 - acc: 0.8570\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 40s 21ms/step - loss: 0.3522 - acc: 0.8696\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 40s 21ms/step - loss: 0.3245 - acc: 0.8785\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3527 - acc: 0.8687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBUIMVSEN82j",
        "outputId": "b853f466-36d7-4e30-c268-d42ea1a1eb8e"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "\r\n",
        "class myCallback(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs={}):\r\n",
        "    if(logs.get('loss')<0.4):\r\n",
        "      print(\"\\nReached 60% accuracy so cancelling training!\")\r\n",
        "      self.model.stop_training = True\r\n",
        "callbacks = myCallback()\r\n",
        "\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\r\n",
        "training_images=training_images / 255.0\r\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\r\n",
        "test_images=test_images/255.0\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\r\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\r\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 5408)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 128)               692352    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 693,962\n",
            "Trainable params: 693,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.5229 - accuracy: 0.8169\n",
            "\n",
            "Reached 60% accuracy so cancelling training!\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3025 - accuracy: 0.8921\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}